nb.of.word.occu <- c()
nb.of.stop.word <- c()
accura <- 2
for(i in 1:(10^accura)) {
#i <- 2
token_word <- c()
extrait.length <- round(i*(0.1^accura)*book.length)
original_books2 <- as_tibble(my.texte[1:extrait.length,1:2])
tokenizer.sentence.i <- sprintf("tokenizer.sentence.%d(original_books2)", choose_tokenizer_sentence)
token_sentence <- eval(parse(text=tokenizer.sentence.i))#[[1]][1]
tokenizer.word.i <- sprintf("tokenizer.word.%d(token_sentence[k,])", choose_tokenizer_word)
for(k in 1:dim(token_sentence)[1]) {
#k = 1
new_token_word <- eval(parse(text=tokenizer.word.i))
token_word <- dplyr::bind_rows(token_word,new_token_word)
}
nb.of.word.occu[i] <- nrow(token_word)
stop_word <- stop.word.1(token_word)
nb.of.stop.word[i] <- nrow(stop_word)
}
return(c(list(nb.of.word.occu), list(nb.of.stop.word)))
}
a = heaps.law(original_books_bis, 1, 1)
nb.of.word.occu = a[[1]]
nb.of.stop.word = a[[2]]
#24, 25,26
#jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/log_heaps_law_data_%d.jpg',choose_load_data),sep =""))
plot(log(nb.of.word.occu),log(nb.of.stop.word))
reg_lin <- lm(log(nb.of.stop.word) ~ log(nb.of.word.occu))
abline(reg_lin)
#dev.off()
K <- exp(reg_lin$coefficients[[1]]) #normalement entre 30 et 100
beta <- reg_lin$coefficients[[2]]   #environ 0.5
summary(reg_lin)
#jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/heaps_law_data_%d.jpg',choose_load_data),sep =""))
plot(nb.of.word.occu, nb.of.stop.word)
lines(nb.of.word.occu, K*nb.of.word.occu^beta, col="red")
#dev.off()
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
original_books_tokenized()[[3]] <- arrange(original_books_tokenized()[[3]], freq)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
efijfenjfenijfrnifrzeinqs <- arrange(after.choose.token(original_books_bis,1,1)[[3]], freq)
efijfenjfenijfrnifrzeinqs
efijfenjfenijfrnifrzeinqs <- arrange(after.choose.token(original_books_bis,1,1)[[3]], freq, desc =TRUE)
efijfenjfenijfrnifrzeinqs <- arrange(after.choose.token(original_books_bis,1,1)[[3]], freq)
efijfenjfenijfrnifrzeinqs <- desc(arrange(after.choose.token(original_books_bis,1,1)[[3]], freq))
efijfenjfenijfrnifrzeinqs <- arrange(after.choose.token(original_books_bis,1,1)[[3]], desc(freq))
efijfenjfenijfrnifrzeinqs
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
gsub("a", "", "vefveoavea")
gsub("a.", "", "vefveoavea")
gsub(": [0-9]", "", "word : 41")
gsub(": [0-9]+", "", "word : 41")
gsub(" : [0-9]+", "", "word : 41")
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
original_books_bis
subset(original_book_bis, text = "SENSE AND SENSIBILITY")
origin
subset(original_books_bis, text = "SENSE AND SENSIBILITY")
subset(original_books_bis, text == "SENSE AND SENSIBILITY")
subset(original_books_bis, text == "SENSE AND SENSIBILITY")$chapter
subset(original_books_bis, text == "SENSE AND SENSIBILITY")$rowname
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
zfrjkkjbqeri <- c(1,4,2,5)
zfrjkkjbqeri
zfrjkkjbqeri <- c(c(1),c(4),c(2),c(5))
zfrjkkjbqeri
zfrjkkjbqeri <- c(list(1),list(4),list(2),list(5))
zfrjkkjbqeri
zfrjkkjbqeri[[i]]
zfrjkkjbqeri[i]
zfrjkkjbqeri[[1]]
zfrjkkjbqeri[1]
agrgr <- c()
for(i in zfrjkkjbqeri){agrgr <- c(agrgr, i)}
agrgr
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
original_books_tokenized_test <- after.choose.token(original_books_bis,1,1)
list_sentences_wordcloud_filter_test <- subset(original_books_tokenized_test[3], word == original_books_tokenized_test[3]$word[1])$sentences
local_data_selected_sentences_wordcloud_test <- c()
for(i in list_sentences_wordcloud_filter_test){
local_data_selected_sentences_wordcloud_test <- c(local_data_selected_sentences_wordcloud_test , original_books_tokenized_test[1][i]$sentence)
}
data.frame(sentence = local_data_selected_sentences_wordcloud_test)
original_books_tokenized_test[3]$word[1]
original_books_tokenized_test[3]$word
original_books_tokenized_test[3]
original_books_tokenized_test[3]$word
original_books_tokenized_test[[3]]$word
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
original_books_tokenized_test <- after.choose.token(original_books_bis,1,1)
list_sentences_wordcloud_filter_test <- subset(original_books_tokenized_test[3], word == original_books_tokenized_test[[3]]$word[1])$sentences
local_data_selected_sentences_wordcloud_test <- c()
for(i in list_sentences_wordcloud_filter_test){
local_data_selected_sentences_wordcloud_test <- c(local_data_selected_sentences_wordcloud_test , original_books_tokenized_test[[1]][i]$sentence)
}
data.frame(sentence = local_data_selected_sentences_wordcloud_test)
original_books_tokenized_test[[3]]$word
original_books_tokenized_test[[3]]$word[1]
original_books_tokenized_test[[3]]$word[100]
list_sentences_wordcloud_filter_test
subset(original_books_tokenized_test[3], word == original_books_tokenized_test[[3]]$word[100])
original_books_tokenized_test[[3]]$word[100]
original_books_tokenized_test[[3]]
list_sentences_wordcloud_filter_test <- filter(original_books_tokenized_test[3], word == original_books_tokenized_test[[3]]$word[100])$sentences
typeof(original_books_tokenized_test[3])
list_sentences_wordcloud_filter_test <- filter(original_books_tokenized_test[3], word == original_books_tokenized_test[[3]]$word[100])$sentences
subset(original_books_tokenized_test[3], word == original_books_tokenized_test[[3]]$word[100])$sentences
subset(original_books_tokenized_test[3], word == "a")
list_sentences_wordcloud_filter_test <- subset(original_books_tokenized_test[[3]], word == original_books_tokenized_test[[3]]$word[100])$sentences
list_sentences_wordcloud_filter_test
original_books_tokenized_test <- after.choose.token(original_books_bis,1,1)
list_sentences_wordcloud_filter_test <- subset(original_books_tokenized_test[[3]], word == original_books_tokenized_test[[3]]$word[100])$sentences
local_data_selected_sentences_wordcloud_test <- c()
for(i in list_sentences_wordcloud_filter_test){
local_data_selected_sentences_wordcloud_test <- c(local_data_selected_sentences_wordcloud_test , original_books_tokenized_test[[1]][i]$sentence)
}
data.frame(sentence = local_data_selected_sentences_wordcloud_test)
riginal_books_tokenized_test[[1]]
original_books_tokenized_test[[1]]
original_books_tokenized_test[[1]]$sentences
original_books_tokenized_test[[1]]$sentence
original_books_tokenized_test[[1]][2]$sentence
original_books_tokenized_test[[1]]$sentence[2]
original_books_tokenized_test <- after.choose.token(original_books_bis,1,1)
list_sentences_wordcloud_filter_test <- subset(original_books_tokenized_test[[3]], word == original_books_tokenized_test[[3]]$word[100])$sentences
local_data_selected_sentences_wordcloud_test <- c()
for(i in list_sentences_wordcloud_filter_test){
local_data_selected_sentences_wordcloud_test <- c(local_data_selected_sentences_wordcloud_test , original_books_tokenized_test[[1]]$sentence[i])
}
data.frame(sentence = local_data_selected_sentences_wordcloud_test)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
zipfs.law <- function(my.texte) {
my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank<0.1*nb.mot, rank > 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(term_frequency) ~ log10(rank), data = rank_subset)
lambda <- exp(reg_lin$coefficients[[1]])
inv <- reg_lin$coefficients[[2]]   #environ -1
summary(reg_lin)
jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/zipfs_law_data_%d.jpg',choose_load_data),sep =""))
freq_by_rank %>% ggplot(aes(rank, term_frequency)) +
geom_abline(intercept = reg_lin$coefficients[[1]], slope = inv, color = "red") +
geom_line(size = 1.1, alpha = 0.8, show.legend= FALSE) +
scale_x_log10() +
scale_y_log10()
dev.off()
}
#zipfs.law(token_word_freq)
zipfs.law(original_text_bis)
View(original_books_tokenized_test)
zipfs.law(original_books_tokenized_test[[3]])
zipfs.law(token_word_freq)
my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank<0.1*nb.mot, rank > 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(term_frequency) ~ log10(rank), data = rank_subset)
term_frequency
rank_subset
freq_by_rank
log10(3)
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank), data = rank_subset)
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank))
all(is.na(rank_subset$term_frequency))
rank_subset$term_frequency
rank_subset
rank_subset <- freq_by_rank %>% filter()
rank_subset
rank_subset <- freq_by_rank %>% filter(rank<0.1*nb.mot)
rank_subset
0.1*nb.mot
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank> 0.1*nb.mot, rank < 0.9*nb.mot) # entre 10% et 90% peut être changer ?
rank_subset
my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank> 0.1*nb.mot, rank < 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank))
lambda <- exp(reg_lin$coefficients[[1]])
inv <- reg_lin$coefficients[[2]]   #environ -1
summary(reg_lin)
jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/zipfs_law_data_%d.jpg',choose_load_data),sep =""))
freq_by_rank %>% ggplot(aes(rank, term_frequency)) +
geom_abline(intercept = reg_lin$coefficients[[1]], slope = inv, color = "red") +
geom_line(size = 1.1, alpha = 0.8, show.legend= FALSE) +
scale_x_log10() +
scale_y_log10()
dev.off()
zipfs.law(token_word_freq)
zipfs.law(token_word_freq)
zipfs.law <- function(my.texte) {
#my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank> 0.1*nb.mot, rank < 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank))
lambda <- exp(reg_lin$coefficients[[1]])
inv <- reg_lin$coefficients[[2]]   #environ -1
summary(reg_lin)
jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/zipfs_law_data_%d.jpg',choose_load_data),sep =""))
freq_by_rank %>% ggplot(aes(rank, term_frequency)) +
geom_abline(intercept = reg_lin$coefficients[[1]], slope = inv, color = "red") +
geom_line(size = 1.1, alpha = 0.8, show.legend= FALSE) +
scale_x_log10() +
scale_y_log10()
dev.off()
}
zipfs.law(token_word_freq)
token_word_freq
zipfs.law <- function(my.texte) {
#my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank> 0.1*nb.mot, rank < 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank))
lambda <- exp(reg_lin$coefficients[[1]])
inv <- reg_lin$coefficients[[2]]   #environ -1
summary(reg_lin)
return(c(list(freq_by_rank), list(lambda, inv), list(summary(reg_lin))))
#
# jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/zipfs_law_data_%d.jpg',choose_load_data),sep =""))
# freq_by_rank %>% ggplot(aes(rank, term_frequency)) +
#   geom_abline(intercept = reg_lin$coefficients[[1]], slope = inv, color = "red") +
#   geom_line(size = 1.1, alpha = 0.8, show.legend= FALSE) +
#   scale_x_log10() +
#   scale_y_log10()
# dev.off()
}
#zipfs.law(token_word_freq)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
zipfs.law(token_word_freq)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
View(original_books_tokenized_test)
zips_law_result_test <- zipfs.law(original_books_tokenized_test[[3]])
zips_law_result_test
zips_law_result_test[[1]]
zipfs.law <- function(my.texte) {
#my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank> 0.1*nb.mot, rank < 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank))
lambda <- exp(reg_lin$coefficients[[1]])
inv <- reg_lin$coefficients[[2]]   #environ -1
summary(reg_lin)
return(c(list(freq_by_rank), list(lambda), list(inv), list(summary(reg_lin))))
#
# jpeg(paste(my_path, sprintf('/Intership_NLP_CU/boxplot/zipfs_law_data_%d.jpg',choose_load_data),sep =""))
# freq_by_rank %>% ggplot(aes(rank, term_frequency)) +
#   geom_abline(intercept = reg_lin$coefficients[[1]], slope = inv, color = "red") +
#   geom_line(size = 1.1, alpha = 0.8, show.legend= FALSE) +
#   scale_x_log10() +
#   scale_y_log10()
# dev.off()
}
#zipfs.law(token_word_freq)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
my.texte <- token_word_freq
total <- sum(my.texte$freq)
nb.mot <- nrow(my.texte)
freq_by_rank <- my.texte %>% mutate(rank = row_number(), term_frequency = freq/total)
#on ne prend que la partie du milieu car c est la plus lineaire
rank_subset <- freq_by_rank %>% filter(rank> 0.1*nb.mot, rank < 0.9*nb.mot) # entre 10% et 90% peut être changer ?
reg_lin <- lm(log10(rank_subset$term_frequency) ~ log10(rank_subset$rank))
lambda <- exp(reg_lin$coefficients[[1]])
inv <- reg_lin$coefficients[[2]]   #environ -1
summary(reg_lin)
freq_by_rank %>% ggplot(aes(rank, term_frequency)) +
geom_abline(intercept = reg_lin$coefficients[[1]], slope = inv, color = "red") +
geom_line(size = 1.1, alpha = 0.8, show.legend= FALSE) +
scale_x_log10() +
scale_y_log10()
zips_law_result_test[[4]]
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
runApp("C:/Users/Projet/Intership_NLP_CU/app", launch.browser = TRUE)
################################ USER CHANGE ############################################
#write your path to go to your file
#my_path <- "C:/Users/Projet/Intership_NLP_CU"
my_path <- "C:/Users/Projet"
#choose which data you want to load
choose_load_data <- 1
DEBUG = TRUE
################################### LOAD DATA ###########################################
source(paste(my_path, sprintf("/Intership_NLP_CU/load_data/load_data_%d.R", choose_load_data), sep = ""))
load.data.i <- sprintf("load.data.%d()", choose_load_data)
original_books <- eval(parse(text=load.data.i))
original_books <- original_books %>% mutate(rowname = 1:nrow(original_books))
original_books_bis <- original_books[1:400,]
#Removing the spaces from the column book. I had to change the type to character in order to change the column and
#then re change it in factor to let it as it was before.
count = 1
original_books_bis$book <- as.character(original_books_bis$book)
for(i in original_books_bis$book){
original_books_bis$book[count] <- gsub(" ", "",i)
count = count +1
}
original_books_bis$book <- as.factor(original_books_bis$book)
n <- NROW(original_books_bis)
################################# TOKENIZER SENTENCE_WORD ###################################
n.tokenizer.sentence <- length(list.files(paste(my_path,"/Intership_NLP_CU/preprocessing/tokenizer_sentence/", sep = "")))
n.tokenizer.word.occu <- length(list.files(paste(my_path,"/Intership_NLP_CU/preprocessing/tokenizer_word_occu/", sep=""))) - 1
nb.of.sentence <- c()
nb.of.word.occu <- c()
nb.of.word.type <- c()
token_word <- c()
for (i in 1:n.tokenizer.sentence){
i = 1 #2,3
lien <- paste(my_path, sprintf("/Intership_NLP_CU/preprocessing/tokenizer_sentence/tokenizer_sentence_%d.R", i), sep = "")
source(lien)
tokenizer.sentence.i <- sprintf("tokenizer.sentence.%d(original_books_bis)", i)
if (DEBUG == TRUE) { print(tokenizer.sentence.i) }
token_sentence <- eval(parse(text=tokenizer.sentence.i))#[[1]][1]
if (DEBUG == TRUE) { print(token_sentence) }
nb.of.sentence[i] <- dim(token_sentence)[1]
if (DEBUG == TRUE) { print(nb.of.sentence) }
#token_word <- c()
for (j in 1:n.tokenizer.word.occu){
token_word <- c()
j = 3 #3,4,5
if(i==1) {
lien <- paste(my_path,sprintf("/Intership_NLP_CU/preprocessing/tokenizer_word_occu/tokenizer_word_occu_%d.R", j), sep="")
source(lien)
}
tokenizer.word.i <- sprintf("tokenizer.word.%d(token_sentence[k,])", j)
if (DEBUG == TRUE) { print(tokenizer.word.i) }
for(k in 1:nb.of.sentence[i]) {
#k = 2  #2,3,4...15773
if (DEBUG == TRUE) { token_sentence[k,] }
new_token_word <- eval(parse(text=tokenizer.word.i))
if (DEBUG == TRUE) { new_token_word }
token_word <- dplyr::bind_rows(token_word,new_token_word) #TODO mettre bout à bout des matrice
if (DEBUG == TRUE) { token_word }
#nb.of.word.occu[i]  <- sum(token_word[2])
#nb.of.word.type[i]  <- dim(token_word[2])[1]
}
###TO DO NOMBREs de mots a verifier 464,193, 464,194
## TO DO a mettre
token_word_sort <- token_word %>% arrange(word)
pre_curseur <- 1
curseur <- 1
token_word_freq <- c()
while(curseur < nrow(token_word_sort)) {
while(identical(token_word_sort$word[pre_curseur], token_word_sort$word[curseur])) {
curseur <- curseur + 1
}
list_sentence <- unique(token_word_sort[[2]][pre_curseur:(curseur - 1)])
freq <- curseur - pre_curseur
token_word_freq1 <- list(word = token_word_sort[[1]][pre_curseur], sentences = list(list_sentence), freq = freq)
names(token_word_freq1) <- c("word","sentences", "freq")
token_word_freq <- dplyr::bind_rows(token_word_freq, as_tibble(token_word_freq1))
pre_curseur <- curseur
token_word_freq
}
}
}
####################################   Doing the choices for the check boxes  ##########################################################
book_unique <- unique(original_books_bis$book)
check_choices <- c()
check_choices_token_sentence_check <- c()
check_choices_token_word_check <- c()
for(i in book_unique){
a_paste_local <- paste("Book", i, sep = "" )
check_choices <- c(check_choices, a_paste_local)
}
count_1 = 1
while(count_1 <= n.tokenizer.sentence){
b_paste_local <- paste("TokenizationSentence", count_1, sep = "" )
check_choices_token_sentence_check <- c(check_choices_token_sentence_check, b_paste_local)
count_1 =count_1 +1
}
count_2 = 1
while(count_2 <= n.tokenizer.word.occu){
c_paste_local <- paste("TokenizationWord", count_2, sep = "" )
check_choices_token_word_check <- c(check_choices_token_word_check, c_paste_local)
count_2 =count_2 +1
}
m <- 400
library(stringi)
################################ USER CHANGE ############################################
#write your path to go to your file
#my_path <- "C:/Users/Projet/Intership_NLP_CU"
my_path <- "C:/Users/Projet"
#choose which data you want to load
choose_load_data <- 1
DEBUG = TRUE
################################### LOAD DATA ###########################################
source(paste(my_path, sprintf("/Intership_NLP_CU/load_data/load_data_%d.R", choose_load_data), sep = ""))
load.data.i <- sprintf("load.data.%d()", choose_load_data)
original_books <- eval(parse(text=load.data.i))
original_books <- original_books %>% mutate(rowname = 1:nrow(original_books))
original_books_bis <- original_books[1:400,]
#Removing the spaces from the column book. I had to change the type to character in order to change the column and
#then re change it in factor to let it as it was before.
count = 1
original_books_bis$book <- as.character(original_books_bis$book)
for(i in original_books_bis$book){
original_books_bis$book[count] <- gsub(" ", "",i)
count = count +1
}
original_books_bis$book <- as.factor(original_books_bis$book)
n <- NROW(original_books_bis)
################################# TOKENIZER SENTENCE_WORD ###################################
n.tokenizer.sentence <- length(list.files(paste(my_path,"/Intership_NLP_CU/preprocessing/tokenizer_sentence/", sep = "")))
n.tokenizer.word.occu <- length(list.files(paste(my_path,"/Intership_NLP_CU/preprocessing/tokenizer_word_occu/", sep=""))) - 1
nb.of.sentence <- c()
nb.of.word.occu <- c()
nb.of.word.type <- c()
token_word <- c()
for (i in 1:n.tokenizer.sentence){
i = 1 #2,3
lien <- paste(my_path, sprintf("/Intership_NLP_CU/preprocessing/tokenizer_sentence/tokenizer_sentence_%d.R", i), sep = "")
source(lien)
tokenizer.sentence.i <- sprintf("tokenizer.sentence.%d(original_books_bis)", i)
if (DEBUG == TRUE) { print(tokenizer.sentence.i) }
token_sentence <- eval(parse(text=tokenizer.sentence.i))#[[1]][1]
if (DEBUG == TRUE) { print(token_sentence) }
nb.of.sentence[i] <- dim(token_sentence)[1]
if (DEBUG == TRUE) { print(nb.of.sentence) }
#token_word <- c()
for (j in 1:n.tokenizer.word.occu){
token_word <- c()
j = 3 #3,4,5
if(i==1) {
lien <- paste(my_path,sprintf("/Intership_NLP_CU/preprocessing/tokenizer_word_occu/tokenizer_word_occu_%d.R", j), sep="")
source(lien)
}
tokenizer.word.i <- sprintf("tokenizer.word.%d(token_sentence[k,])", j)
if (DEBUG == TRUE) { print(tokenizer.word.i) }
for(k in 1:nb.of.sentence[i]) {
#k = 2  #2,3,4...15773
if (DEBUG == TRUE) { token_sentence[k,] }
new_token_word <- eval(parse(text=tokenizer.word.i))
if (DEBUG == TRUE) { new_token_word }
token_word <- dplyr::bind_rows(token_word,new_token_word) #TODO mettre bout à bout des matrice
if (DEBUG == TRUE) { token_word }
#nb.of.word.occu[i]  <- sum(token_word[2])
#nb.of.word.type[i]  <- dim(token_word[2])[1]
}
###TO DO NOMBREs de mots a verifier 464,193, 464,194
## TO DO a mettre
token_word_sort <- token_word %>% arrange(word)
pre_curseur <- 1
curseur <- 1
token_word_freq <- c()
while(curseur < nrow(token_word_sort)) {
while(identical(token_word_sort$word[pre_curseur], token_word_sort$word[curseur])) {
curseur <- curseur + 1
}
list_sentence <- unique(token_word_sort[[2]][pre_curseur:(curseur - 1)])
freq <- curseur - pre_curseur
token_word_freq1 <- list(word = token_word_sort[[1]][pre_curseur], sentences = list(list_sentence), freq = freq)
names(token_word_freq1) <- c("word","sentences", "freq")
token_word_freq <- dplyr::bind_rows(token_word_freq, as_tibble(token_word_freq1))
pre_curseur <- curseur
token_word_freq
}
}
}
####################################   Doing the choices for the check boxes  ##########################################################
book_unique <- unique(original_books_bis$book)
check_choices <- c()
check_choices_token_sentence_check <- c()
check_choices_token_word_check <- c()
for(i in book_unique){
a_paste_local <- paste("Book", i, sep = "" )
check_choices <- c(check_choices, a_paste_local)
}
count_1 = 1
while(count_1 <= n.tokenizer.sentence){
b_paste_local <- paste("TokenizationSentence", count_1, sep = "" )
check_choices_token_sentence_check <- c(check_choices_token_sentence_check, b_paste_local)
count_1 =count_1 +1
}
count_2 = 1
while(count_2 <= n.tokenizer.word.occu){
c_paste_local <- paste("TokenizationWord", count_2, sep = "" )
check_choices_token_word_check <- c(check_choices_token_word_check, c_paste_local)
count_2 =count_2 +1
}
m <- 400
